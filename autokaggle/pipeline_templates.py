"""Template renderers for local pipeline generation."""

from __future__ import annotations

from typing import Any

from autokaggle.chat_manager import ChatDecision


def render_data_loading(profile: dict[str, Any]) -> str:
    train_file = profile.get("train_file", "train.csv")
    sample_file = profile.get("sample_submission_file", "sample_submission.csv")
    return (
        '"""Utilities for loading competition datasets."""\n\n'
        "from __future__ import annotations\n\n"
        "from pathlib import Path\n\n"
        "import pandas as pd\n\n"
        f"TRAIN_FILE = {train_file!r}\n"
        f"SAMPLE_SUBMISSION_FILE = {sample_file!r}\n\n"
        "\n"
        "def _run_root() -> Path:\n"
        "    return Path(__file__).resolve().parents[1]\n\n"
        "\n"
        "def _input_dir() -> Path:\n"
        "    return _run_root() / 'input'\n\n"
        "\n"
        "def _collect_csv_files() -> list[Path]:\n"
        "    input_dir = _input_dir()\n"
        "    return sorted(input_dir.rglob('*.csv'))\n\n"
        "\n"
        "def _resolve_csv(path_hint: str | None, exclude: set[str]) -> Path:\n"
        "    if path_hint:\n"
        "        candidate = _input_dir() / path_hint\n"
        "        if candidate.exists():\n"
        "            return candidate\n"
        "    for path in _collect_csv_files():\n"
        "        name = path.name.lower()\n"
        "        if name in exclude or 'sample_submission' in name:\n"
        "            continue\n"
        "        return path\n"
        "    raise FileNotFoundError('No suitable CSV found in input directory.')\n\n"
        "\n"
        "def _find_sample_submission_file() -> Path | None:\n"
        "    if SAMPLE_SUBMISSION_FILE:\n"
        "        candidate = _input_dir() / SAMPLE_SUBMISSION_FILE\n"
        "        if candidate.exists():\n"
        "            return candidate\n"
        "    for path in _collect_csv_files():\n"
        "        if 'sample_submission' in path.name.lower():\n"
        "            return path\n"
        "    return None\n\n"
        "\n"
        "def load_training_data() -> pd.DataFrame:\n"
        "    excludes = {Path('test.csv').name}\n"
        "    if SAMPLE_SUBMISSION_FILE:\n"
        "        excludes.add(Path(SAMPLE_SUBMISSION_FILE).name)\n"
        "    path = _resolve_csv(TRAIN_FILE, excludes)\n"
        "    return pd.read_csv(path)\n\n"
        "\n"
        "def load_test_data() -> pd.DataFrame | None:\n"
        "    input_dir = _input_dir()\n"
        "    preferred = input_dir / 'test.csv'\n"
        "    if preferred.exists():\n"
        "        return pd.read_csv(preferred)\n"
        "    train_name = Path(TRAIN_FILE).name.lower()\n"
        "    sample_name = Path(SAMPLE_SUBMISSION_FILE).name.lower() if SAMPLE_SUBMISSION_FILE else ''\n"
        "    for path in _collect_csv_files():\n"
        "        name = path.name.lower()\n"
        "        if name in {train_name, sample_name} or 'sample_submission' in name:\n"
        "            continue\n"
        "        return pd.read_csv(path)\n"
        "    return None\n\n"
        "\n"
        "def load_sample_submission() -> pd.DataFrame | None:\n"
        "    path = _find_sample_submission_file()\n"
        "    if path is None:\n"
        "        return None\n"
        "    return pd.read_csv(path)\n"
    )


def render_preprocess(profile: dict[str, Any]) -> str:
    numeric_columns = profile.get("numeric_columns", [])
    categorical_columns = profile.get("categorical_columns", [])
    return (
        '"""Preprocessing helpers."""\n\n'
        "from __future__ import annotations\n\n"
        "import json\n"
        "from pathlib import Path\n\n"
        "from sklearn.compose import ColumnTransformer\n"
        "from sklearn.impute import SimpleImputer\n"
        "from sklearn.pipeline import Pipeline\n"
        "from sklearn.preprocessing import OneHotEncoder\n\n"
        f"DEFAULT_NUMERIC_COLUMNS = {numeric_columns!r}\n"
        f"DEFAULT_CATEGORICAL_COLUMNS = {categorical_columns!r}\n\n"
        "\n"
        "def _run_root() -> Path:\n"
        "    return Path(__file__).resolve().parents[1]\n\n"
        "\n"
        "def load_profile() -> dict:\n"
        "    profile_path = _run_root() / 'input' / 'data_profile.json'\n"
        "    return json.loads(profile_path.read_text())\n\n"
        "\n"
        "def build_preprocessor(profile: dict | None = None) -> ColumnTransformer:\n"
        "    if profile is None:\n"
        "        profile = load_profile()\n"
        "    numeric = profile.get('numeric_columns') or DEFAULT_NUMERIC_COLUMNS\n"
        "    categorical = profile.get('categorical_columns') or DEFAULT_CATEGORICAL_COLUMNS\n"
        "    targets = set(profile.get('target_inference', {}).get('columns') or [])\n"
        "    if targets:\n"
        "        numeric = [col for col in numeric if col not in targets]\n"
        "        categorical = [col for col in categorical if col not in targets]\n\n"
        "    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n"
        "    categorical_transformer = Pipeline(\n"
        "        steps=[\n"
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n"
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')),\n"
        "        ]\n"
        "    )\n\n"
        "    return ColumnTransformer(\n"
        "        transformers=[\n"
        "            ('num', numeric_transformer, numeric),\n"
        "            ('cat', categorical_transformer, categorical),\n"
        "        ],\n"
        "        remainder='drop',\n"
        "    )\n"
    )


def render_train(profile: dict[str, Any]) -> str:
    target_columns = profile.get("target_inference", {}).get("columns", [])
    target_hint = target_columns[0] if target_columns else "target"
    return (
        '"""Train ensemble models."""\n\n'
        "from __future__ import annotations\n\n"
        "import json\n"
        "from pathlib import Path\n\n"
        "import joblib\n"
        "import numpy as np\n"
        "from sklearn.pipeline import Pipeline\n\n"
        "from data_loading import load_training_data\n"
        "from preprocess import build_preprocessor, load_profile\n"
        "from strategy import CATBOOST_PARAMS, LIGHTGBM_PARAMS, XGBOOST_PARAMS\n\n"
        f"TARGET_FALLBACK = {target_hint!r}\n\n"
        "\n"
        "def _run_root() -> Path:\n"
        "    return Path(__file__).resolve().parents[1]\n\n"
        "\n"
        "def _infer_target(profile: dict) -> str:\n"
        "    targets = profile.get('target_inference', {}).get('columns') or []\n"
        "    if targets:\n"
        "        return targets[0]\n"
        "    return TARGET_FALLBACK\n\n"
        "\n"
        "def _is_classification(values: np.ndarray, dtype: str) -> bool:\n"
        "    if dtype in {'object', 'bool'}:\n"
        "        return True\n"
        "    unique = np.unique(values)\n"
        "    return unique.size <= 20\n\n"
        "\n"
        "def _merge_params(defaults: dict, custom: dict | None) -> dict:\n"
        "    params = dict(defaults)\n"
        "    if custom:\n"
        "        params.update(custom)\n"
        "    return params\n\n"
        "\n"
        "def _build_models(is_classification: bool) -> dict[str, object]:\n"
        "    from lightgbm import LGBMClassifier, LGBMRegressor\n"
        "    from xgboost import XGBClassifier, XGBRegressor\n"
        "    from catboost import CatBoostClassifier, CatBoostRegressor\n\n"
        "    models: dict[str, object] = {}\n"
        "    lgb_defaults = {'n_estimators': 300}\n"
        "    xgb_defaults = {'n_estimators': 300, 'tree_method': 'hist'}\n"
        "    cat_defaults = {'iterations': 300, 'verbose': False}\n\n"
        "    if is_classification:\n"
        "        models['lightgbm'] = LGBMClassifier(**_merge_params(lgb_defaults, LIGHTGBM_PARAMS))\n"
        "        models['xgboost'] = XGBClassifier(**_merge_params(xgb_defaults, XGBOOST_PARAMS))\n"
        "        models['catboost'] = CatBoostClassifier(**_merge_params(cat_defaults, CATBOOST_PARAMS))\n"
        "    else:\n"
        "        models['lightgbm'] = LGBMRegressor(**_merge_params(lgb_defaults, LIGHTGBM_PARAMS))\n"
        "        models['xgboost'] = XGBRegressor(**_merge_params(xgb_defaults, XGBOOST_PARAMS))\n"
        "        models['catboost'] = CatBoostRegressor(**_merge_params(cat_defaults, CATBOOST_PARAMS))\n"
        "    return models\n\n"
        "\n"
        "def train() -> dict[str, Path]:\n"
        "    profile = load_profile()\n"
        "    target_column = _infer_target(profile)\n"
        "    train_df = load_training_data()\n\n"
        "    if target_column not in train_df.columns:\n"
        "        raise ValueError(f'Target column {target_column} not found in training data.')\n\n"
        "    y = train_df[target_column].values\n"
        "    X = train_df.drop(columns=[target_column])\n\n"
        "    target_schema = profile.get('schema', {}).get(target_column, {})\n"
        "    dtype = str(target_schema.get('dtype', 'object'))\n"
        "    is_classification = _is_classification(y, dtype)\n\n"
        "    preprocessor = build_preprocessor(profile)\n"
        "    model_map = _build_models(is_classification)\n\n"
        "    output_dir = _run_root() / 'output'\n"
        "    output_dir.mkdir(parents=True, exist_ok=True)\n\n"
        "    model_paths: dict[str, Path] = {}\n"
        "    for name, model in model_map.items():\n"
        "        pipeline = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])\n"
        "        pipeline.fit(X, y)\n"
        "        model_path = output_dir / f'model_{name}.joblib'\n"
        "        joblib.dump(pipeline, model_path)\n"
        "        model_paths[name] = model_path\n\n"
        "    metadata = {\n"
        "        'is_classification': is_classification,\n"
        "        'target_column': target_column,\n"
        "        'model_files': {name: path.name for name, path in model_paths.items()},\n"
        "    }\n"
        "    metadata_path = output_dir / 'model_meta.json'\n"
        "    metadata_path.write_text(json.dumps(metadata, indent=2))\n\n"
        "    return model_paths\n\n"
        "\n"
        "if __name__ == '__main__':\n"
        "    train()\n"
    )


def render_predict(profile: dict[str, Any]) -> str:
    target_columns = profile.get("target_inference", {}).get("columns", [])
    return (
        '"""Generate predictions and a submission file."""\n\n'
        "from __future__ import annotations\n\n"
        "from pathlib import Path\n\n"
        "import json\n"
        "import joblib\n"
        "import numpy as np\n\n"
        "from data_loading import load_sample_submission, load_test_data\n"
        "from preprocess import load_profile\n"
        "from strategy import EVALUATION_METRIC\n"
        "from train import _infer_target\n\n"
        f"TARGET_COLUMNS = {target_columns!r}\n\n"
        "\n"
        "def _run_root() -> Path:\n"
        "    return Path(__file__).resolve().parents[1]\n\n"
        "\n"
        "def _normalize_metric(metric: str) -> str:\n"
        "    metric = metric.lower().strip()\n"
        "    if 'auc' in metric:\n"
        "        return 'roc_auc'\n"
        "    if 'logloss' in metric or 'log loss' in metric:\n"
        "        return 'log_loss'\n"
        "    return metric\n\n"
        "\n"
        "def _load_metadata(output_dir: Path) -> dict:\n"
        "    metadata_path = output_dir / 'model_meta.json'\n"
        "    if metadata_path.exists():\n"
        "        return json.loads(metadata_path.read_text())\n"
        "    return {}\n\n"
        "\n"
        "def _load_models(output_dir: Path, model_files: dict[str, str]) -> dict[str, object]:\n"
        "    models: dict[str, object] = {}\n"
        "    for name, filename in model_files.items():\n"
        "        path = output_dir / filename\n"
        "        if not path.exists():\n"
        "            raise FileNotFoundError(f'Model not found: {path}')\n"
        "        models[name] = joblib.load(path)\n"
        "    return models\n\n"
        "\n"
        "def _majority_vote(predictions: np.ndarray) -> np.ndarray:\n"
        "    def _vote(row: np.ndarray):\n"
        "        values, counts = np.unique(row, return_counts=True)\n"
        "        return values[np.argmax(counts)]\n"
        "    return np.apply_along_axis(_vote, axis=1, arr=predictions)\n\n"
        "\n"
        "def predict() -> Path:\n"
        "    run_root = _run_root()\n"
        "    output_dir = run_root / 'output'\n"
        "    metadata = _load_metadata(output_dir)\n"
        "    model_files = metadata.get(\n"
        "        'model_files',\n"
        "        {\n"
        "            'lightgbm': 'model_lightgbm.joblib',\n"
        "            'xgboost': 'model_xgboost.joblib',\n"
        "            'catboost': 'model_catboost.joblib',\n"
        "        },\n"
        "    )\n"
        "    models = _load_models(output_dir, model_files)\n"
        "    test_df = load_test_data()\n"
        "    if test_df is None:\n"
        "        raise FileNotFoundError('No test data found for prediction.')\n\n"
        "    metric_name = _normalize_metric(EVALUATION_METRIC)\n"
        "    is_classification = bool(metadata.get('is_classification', False))\n"
        "    use_probabilities = is_classification and metric_name in {'log_loss', 'roc_auc'}\n"
        "    if use_probabilities:\n"
        "        probas = []\n"
        "        for model in models.values():\n"
        "            if not hasattr(model, 'predict_proba'):\n"
        "                raise ValueError('Model missing predict_proba for probability-based ensemble.')\n"
        "            probas.append(model.predict_proba(test_df))\n"
        "        predictions = np.mean(np.stack(probas), axis=0)\n"
        "    else:\n"
        "        preds = [model.predict(test_df) for model in models.values()]\n"
        "        stacked = np.column_stack(preds)\n"
        "        if is_classification:\n"
        "            predictions = _majority_vote(stacked)\n"
        "        else:\n"
        "            predictions = np.mean(stacked, axis=1)\n"
        "    sample = load_sample_submission()\n\n"
        "    if sample is None:\n"
        "        profile = load_profile()\n"
        "        target_column = _infer_target(profile)\n"
        "        sample = test_df[[test_df.columns[0]]].copy()\n"
        "        sample.columns = ['id']\n"
        "        if use_probabilities and getattr(predictions, 'ndim', 1) > 1:\n"
        "            sample[target_column] = predictions[:, -1]\n"
        "        else:\n"
        "            sample[target_column] = predictions\n"
        "    else:\n"
        "        target_cols = [col for col in sample.columns if col.lower() not in {'id', 'index'}]\n"
        "        if len(target_cols) == 1:\n"
        "            if use_probabilities and getattr(predictions, 'ndim', 1) > 1:\n"
        "                sample[target_cols[0]] = predictions[:, -1]\n"
        "            else:\n"
        "                sample[target_cols[0]] = predictions\n"
        "        else:\n"
        "            for idx, col in enumerate(target_cols):\n"
        "                sample[col] = predictions[:, idx]\n\n"
        "    output_dir.mkdir(parents=True, exist_ok=True)\n"
        "    submission_path = output_dir / 'submission.csv'\n"
        "    sample.to_csv(submission_path, index=False)\n"
        "    return submission_path\n\n"
        "\n"
        "if __name__ == '__main__':\n"
        "    predict()\n"
    )

def render_requirements(decision: ChatDecision) -> str:
    base = [
        "pandas>=2.2.2",
        "numpy>=1.26.4",
        "scikit-learn>=1.4.2",
        "joblib>=1.3.2",
        "lightgbm>=4.3.0",
        "xgboost>=2.0.3",
        "catboost>=1.2.3",
    ]
    return "\n".join(base) + "\n"
